# -*- coding: utf-8 -*-
"""customPCN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16RsUyW1wrhi1vM9RzNgZe4laWBeLF-uu
"""

!apt-get update
!apt-get install -y nvidia-driver-535

!apt-get install ninja-build
!nvidia-smi

import torch
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA device name:", torch.cuda.get_device_name(0))

!gdown --id 1fMnSglDvuGp2QvuzmI8Af8d-CQeQdrEG --output Mesh_Recreated_Original.zip
!gdown --id 1Y6Qvs8hYqWcK0d7hszn5vurw7c7UI20Y --output Mesh_Recreated_Holes.zip

import os

# directory containing the zip files
os.chdir('/content')

!unzip -q Mesh_Recreated_Original.zip -d Mesh_Recreated_Original
!unzip -q Mesh_Recreated_Holes.zip -d Mesh_Recreated_Holes

print("Files unzipped successfully!")

"""Trained with chamfer distance- unused now"""

import os, glob, torch, trimesh
import numpy as np
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#### Dataset + Normalize

def normalize(pc):
    pc = pc - pc.mean(axis=1, keepdims=True)  # center
    scale = pc.norm(dim=0).max()
    pc = pc / scale
    return pc

class CachedPointCloudDataset(Dataset):
    def __init__(self, incomplete_paths, complete_paths, num_points=2048, cache_dir="/content/cached_pointclouds"):
        self.incomplete_paths = incomplete_paths
        self.complete_paths = complete_paths
        self.num_points = num_points
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.inc_pc_paths, self.gt_pc_paths = self._prepare_cached_pointclouds()

    def _cache_exists_and_valid(self, path, expected_shape):
        return os.path.exists(path) and np.load(path).shape == expected_shape

    def _prepare_cached_pointclouds(self):
        inc_cached = []
        gt_cached = []

        expected_shape = (3, self.num_points)

        for inc_path, gt_path in zip(self.incomplete_paths, self.complete_paths):
            base = os.path.splitext(os.path.basename(inc_path))[0]
            inc_pc_file = os.path.join(self.cache_dir, f"{base}_inc.npy")
            gt_pc_file = os.path.join(self.cache_dir, f"{base}_gt.npy")

            if not self._cache_exists_and_valid(inc_pc_file, expected_shape):
                inc_mesh = trimesh.load(inc_path, force='mesh')
                inc_pts, _ = trimesh.sample.sample_surface(inc_mesh, self.num_points)
                np.save(inc_pc_file, inc_pts.T)

            if not self._cache_exists_and_valid(gt_pc_file, expected_shape):
                gt_mesh = trimesh.load(gt_path, force='mesh')
                gt_pts, _ = trimesh.sample.sample_surface(gt_mesh, self.num_points)
                np.save(gt_pc_file, gt_pts.T)

            inc_cached.append(inc_pc_file)
            gt_cached.append(gt_pc_file)

        return inc_cached, gt_cached

    def __len__(self):
        return len(self.inc_pc_paths)

    def __getitem__(self, idx):
        inc = torch.from_numpy(np.load(self.inc_pc_paths[idx])).float()
        gt = torch.from_numpy(np.load(self.gt_pc_paths[idx])).float()
        inc = normalize(inc)
        gt = normalize(gt)
        return inc, gt


#### Model with Skip Connections

class PCNEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Conv1d(3, 128, 1),
            nn.ReLU(),
            nn.Conv1d(128, 256, 1),
            nn.ReLU(),
            nn.Conv1d(256, 1024, 1),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.mlp(x)
        global_feat = torch.max(x, 2)[0]
        return global_feat

class FoldingDecoder(nn.Module):
    def __init__(self, grid_size=32, input_feat_dim=1024):
        super().__init__()
        self.num_fine = grid_size * grid_size
        self.grid = self.build_2d_grid(grid_size)
        self.mlp1 = nn.Sequential(
            nn.Conv1d(input_feat_dim + 2 + 3, 512, 1),
            nn.ReLU(),
            nn.Conv1d(512, 512, 1),
            nn.ReLU(),
            nn.Conv1d(512, 3, 1)
        )

    def build_2d_grid(self, grid_size):
        linspace = torch.linspace(-0.3, 0.3, grid_size)
        grid = torch.stack(torch.meshgrid(linspace, linspace, indexing='ij'), dim=-1).reshape(-1, 2)
        return grid

    def forward(self, coarse, global_feat):
        B, _, M = coarse.size()
        grid = self.grid.to(coarse.device).unsqueeze(0).repeat(B, 1, 1)
        grid = grid.permute(0, 2, 1)

        global_feat = global_feat.unsqueeze(-1).expand(-1, -1, self.num_fine)
        coarse_exp = coarse[:, :, :self.num_fine]

        # SKIP CONNECTION coarse_exp
        x = torch.cat([grid, coarse_exp, global_feat], dim=1)
        fine = self.mlp1(x) + 3*coarse_exp
        return fine

class PCN(nn.Module):
    def __init__(self, num_coarse=1024, grid_size=32):
        super().__init__()
        self.encoder = PCNEncoder()
        self.num_coarse = num_coarse
        self.grid_size = grid_size

        self.coarse_fc = nn.Sequential(
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, num_coarse * 3)
        )

        self.decoder = FoldingDecoder(grid_size=grid_size)

    def forward(self, x):
        global_feat = self.encoder(x)
        coarse = self.coarse_fc(global_feat).view(-1, 3, self.num_coarse)
        fine = self.decoder(coarse, global_feat)
        out = torch.cat([coarse, fine], dim=2)
        return out

#### Loss
def chamfer_distance(pc1, pc2):
    pc1, pc2 = pc1.transpose(1, 2), pc2.transpose(1, 2)
    dist1 = torch.cdist(pc1, pc2).min(dim=2)[0]
    dist2 = torch.cdist(pc2, pc1).min(dim=2)[0]
    return dist1.mean() + dist2.mean()

def repulsion_loss(pc, k=5):
    pc = pc.transpose(1, 2)
    dists = torch.cdist(pc, pc)
    dists, _ = torch.topk(dists, k=k + 1, dim=-1, largest=False)
    return torch.mean(torch.exp(-dists[:, :, 1:]))

#### Visualize
def plot_pointclouds(input_pc, pred_pc, gt_pc, epoch, idx):
    fig = plt.figure(figsize=(12, 4))
    titles = ['Input (Incomplete)', 'Prediction', 'Ground Truth']
    pcs = [input_pc, pred_pc, gt_pc]

    for i in range(3):
        ax = fig.add_subplot(1, 3, i + 1, projection='3d')
        pc = pcs[i].T
        ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
        ax.set_title(titles[i])
        ax.axis('off')
        ax.set_box_aspect([1, 1, 1])
    plt.suptitle(f"Epoch {epoch} Sample {idx}")
    plt.tight_layout()
    plt.show()


#### Train
def train_pcn(model, train_loader, val_set, epochs=15, lr=5e-4):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.train()

    for epoch in range(1, epochs + 1):
        total_loss = 0.0

        for inc, gt in train_loader:
            inc, gt = inc.to(device), gt.to(device)
            pred = model(inc)

            cd_loss = chamfer_distance(pred, gt)
            repulse = repulsion_loss(pred)
            loss = cd_loss + 0.1 * repulse

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch}: Training Loss = {total_loss / len(train_loader):.6f}")

        # Visualize sel validation samples after each epoch
        model.eval()
        with torch.no_grad():
            for i in range(min(3, len(val_set))):
                inc, gt = val_set[i]
                inc_input = inc.unsqueeze(0).to(device)
                pred = model(inc_input)[0].cpu()
                plot_pointclouds(inc, pred, gt, epoch, i)
        model.train()


###Train Test Split
incomplete_paths = sorted(glob.glob('/content/Mesh_Recreated_Holes/Mesh_Recreaded_Holes/*'))
complete_paths = sorted(glob.glob('/content/Mesh_Recreated_Original/Mesh_Recreated_Original/*'))

dataset = CachedPointCloudDataset(incomplete_paths, complete_paths, num_points=2048)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_set, val_set = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_set, batch_size=4, shuffle=True)

#### Instantiate Model/ Train
model = PCN(num_coarse=1024, grid_size=32).to(device)
train_pcn(model, train_loader, val_set, epochs=15, lr=5e-4)

torch.save(model.state_dict(), '/content/pcn_trained_with_skip_norm.pth')

#import matplotlib.pyplot as plt
#from mpl_toolkits.mplot3d import Axes3D
def plot_pair(inc, gt, idx):

    fig = plt.figure(figsize=(8, 4))

    ax1 = fig.add_subplot(1, 2, 1, projection='3d')
    pc = inc.T
    ax1.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
    ax1.set_title(f'Input with Hole #{idx}')
    ax1.axis('off')

    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    pc = gt.T
    ax2.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
    ax2.set_title('Ground Truth')
    ax2.axis('off')

    plt.tight_layout()
    plt.show()

#for idx in range(len(dataset)):
#    inc, gt = dataset[idx]
#    plot_pair(inc, gt, idx)

"""Training with skips, deeper nn..."""

import os, glob, torch, trimesh
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn as nn
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#### Dataset with normalize
def normalize(pc):
    pc = pc - pc.mean(axis=1, keepdims=True)
    scale = pc.norm(dim=0).max()
    pc = pc / scale
    return pc

class CachedPointCloudDataset(Dataset):
    def __init__(self, incomplete_paths, complete_paths, num_points=2048, cache_dir="/content/cached_pointclouds"):
        self.incomplete_paths = incomplete_paths
        self.complete_paths = complete_paths
        self.num_points = num_points
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.inc_pc_paths, self.gt_pc_paths = self._prepare_cached_pointclouds()

    def _cache_exists_and_valid(self, path, expected_shape):
        return os.path.exists(path) and np.load(path).shape == expected_shape

    def _prepare_cached_pointclouds(self):
        inc_cached = []
        gt_cached = []
        expected_shape = (3, self.num_points)

        for inc_path, gt_path in zip(self.incomplete_paths, self.complete_paths):
            base = os.path.splitext(os.path.basename(inc_path))[0]
            inc_pc_file = os.path.join(self.cache_dir, f"{base}_inc.npy")
            gt_pc_file = os.path.join(self.cache_dir, f"{base}_gt.npy")

            if not self._cache_exists_and_valid(inc_pc_file, expected_shape):
                inc_mesh = trimesh.load(inc_path, force='mesh')
                inc_pts, _ = trimesh.sample.sample_surface(inc_mesh, self.num_points)
                np.save(inc_pc_file, inc_pts.T)

            if not self._cache_exists_and_valid(gt_pc_file, expected_shape):
                gt_mesh = trimesh.load(gt_path, force='mesh')
                gt_pts, _ = trimesh.sample.sample_surface(gt_mesh, self.num_points)
                np.save(gt_pc_file, gt_pts.T)

            inc_cached.append(inc_pc_file)
            gt_cached.append(gt_pc_file)

        return inc_cached, gt_cached

    def __len__(self):
        return len(self.inc_pc_paths)

    def __getitem__(self, idx):
        inc = torch.from_numpy(np.load(self.inc_pc_paths[idx])).float()
        gt = torch.from_numpy(np.load(self.gt_pc_paths[idx])).float()
        inc = normalize(inc)
        gt = normalize(gt)
        return inc, gt

#### Model
class PCNEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Conv1d(3, 128, 1),
            nn.ReLU(),
            nn.Conv1d(128, 256, 1),
            nn.ReLU(),
            nn.Conv1d(256, 1024, 1),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.mlp(x)
        global_feat = torch.max(x, 2)[0]
        return global_feat

class FoldingDecoder(nn.Module):
    def __init__(self, grid_size=32, input_feat_dim=1024):
        super().__init__()
        self.num_fine = grid_size * grid_size
        self.grid = self.build_2d_grid(grid_size)
        self.mlp = nn.Sequential(
            nn.Conv1d(input_feat_dim + 2 + 3, 1024, 1),
            nn.ReLU(),
            nn.Conv1d(1024, 1024, 1),
            nn.ReLU(),
            nn.Conv1d(1024, 512, 1),
            nn.ReLU(),
            nn.Conv1d(512, 256, 1),
            nn.ReLU(),
            nn.Conv1d(256, 3, 1)
        )

    def build_2d_grid(self, grid_size):
        linspace = torch.linspace(-0.3, 0.3, grid_size)
        grid = torch.stack(torch.meshgrid(linspace, linspace, indexing='ij'), dim=-1).reshape(-1, 2)
        return grid

    def forward(self, coarse, global_feat):
        B, _, M = coarse.size()
        grid = self.grid.to(coarse.device).unsqueeze(0).repeat(B, 1, 1)
        grid = grid.permute(0, 2, 1)
        global_feat = global_feat.unsqueeze(-1).expand(-1, -1, self.num_fine)
        coarse_exp = coarse[:, :, :self.num_fine]
        x = torch.cat([grid, coarse_exp, global_feat], dim=1)
        fine = self.mlp(x) + 2.0 * coarse_exp  # Strong skip connection
        return fine

class PCN(nn.Module):
    def __init__(self, num_coarse=1024, grid_size=32):
        super().__init__()
        self.encoder = PCNEncoder()
        self.num_coarse = num_coarse
        self.grid_size = grid_size
        self.coarse_fc = nn.Sequential(
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, num_coarse * 3)
        )
        self.decoder = FoldingDecoder(grid_size=grid_size)

    def forward(self, x):
        global_feat = self.encoder(x)
        coarse = self.coarse_fc(global_feat).view(-1, 3, self.num_coarse)
        fine = self.decoder(coarse, global_feat)
        out = torch.cat([coarse, fine], dim=2)
        return out, coarse

#### Losses
def chamfer_distance(pc1, pc2):
    pc1, pc2 = pc1.transpose(1, 2), pc2.transpose(1, 2)
    dist1 = torch.cdist(pc1, pc2).min(dim=2)[0]
    dist2 = torch.cdist(pc2, pc1).min(dim=2)[0]
    return dist1.mean() + dist2.mean()

def repulsion_loss(pc, k=5):
    pc = pc.transpose(1, 2)
    dists = torch.cdist(pc, pc)
    dists, _ = torch.topk(dists, k=k + 1, dim=-1, largest=False)
    return torch.mean(torch.exp(-dists[:, :, 1:]))

def structure_loss(pred, coarse):
    coarse_expanded = torch.cat([coarse, coarse], dim=2)
    return F.mse_loss(pred, coarse_expanded)

#### Visualize
def plot_pointclouds(input_pc, pred_pc, gt_pc, epoch, idx):
    fig = plt.figure(figsize=(12, 4))
    titles = ['Input', 'Prediction', 'Ground Truth']
    pcs = [input_pc, pred_pc, gt_pc]
    for i in range(3):
        ax = fig.add_subplot(1, 3, i + 1, projection='3d')
        pc = pcs[i].T
        ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
        ax.set_title(titles[i])
        ax.axis('off')
        ax.set_box_aspect([1, 1, 1])
    plt.suptitle(f"Epoch {epoch} Sample {idx}")
    plt.tight_layout()
    plt.show()

#### Training
def train_pcn(model, train_loader, val_set, epochs=20, lr=5e-4):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.train()

    for epoch in range(1, epochs + 1):
        total_loss = 0.0
        for inc, gt in train_loader:
            inc, gt = inc.to(device), gt.to(device)
            pred, coarse = model(inc)
            cd = chamfer_distance(pred, gt)
            repulse = repulsion_loss(pred)
            struct = structure_loss(pred, coarse)
            loss = cd + 0.1 * repulse + 0.05 * struct

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch}: Training Loss = {total_loss / len(train_loader):.6f}")

        model.eval()
        with torch.no_grad():
            for i in range(min(3, len(val_set))):
                inc, gt = val_set[i]
                inc_input = inc.unsqueeze(0).to(device)
                pred, _ = model(inc_input)
                pred = pred[0].cpu()
                plot_pointclouds(inc, pred, gt, epoch, i)
        model.train()

#### Dataset loading/ Train
incomplete_paths = sorted(glob.glob('/content/Mesh_Recreated_Holes/Mesh_Recreaded_Holes/*'))
complete_paths = sorted(glob.glob('/content/Mesh_Recreated_Original/Mesh_Recreated_Original/*'))

#assert len(incomplete_paths) == len(complete_paths), "Mismatch in dataset lengths!"

dataset = CachedPointCloudDataset(incomplete_paths, complete_paths, num_points=2048)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_set, val_set = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_set, batch_size=4, shuffle=True)

#### Model Init & Train
model = PCN(num_coarse=1024, grid_size=32).to(device)
train_pcn(model, train_loader, val_set, epochs=20, lr=5e-4)

torch.save(model.state_dict(), '/content/pcn_trained_deep_skip_final.pth')

"""Pick up at checkpoint, continue training"""

# Reload dataset again to be safe
incomplete_paths = sorted(glob.glob('/content/Mesh_Recreated_Holes/Mesh_Recreaded_Holes/*'))
complete_paths = sorted(glob.glob('/content/Mesh_Recreated_Original/Mesh_Recreated_Original/*'))

dataset = CachedPointCloudDataset(incomplete_paths, complete_paths, num_points=2048)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_set, val_set = random_split(dataset, [train_size, val_size])
train_loader = DataLoader(train_set, batch_size=4, shuffle=True)

# Instantiate model
model = PCN(num_coarse=1024, grid_size=32).to(device)

# Load saved weights from epoch 20
model.load_state_dict(torch.load('/content/pcn_trained_deep_skip_final.pth'))
print("Model loaded from epoch 20 checkpoint.")

#### Continue training: Epochs 21 to 50
def continue_training(model, train_loader, val_set, start_epoch=21, end_epoch=50, lr=5e-4):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.train()

    for epoch in range(start_epoch, end_epoch + 1):
        total_loss = 0.0
        for inc, gt in train_loader:
            inc, gt = inc.to(device), gt.to(device)
            pred, coarse = model(inc)
            cd = chamfer_distance(pred, gt)
            repulse = repulsion_loss(pred)
            struct = structure_loss(pred, coarse)
            loss = cd + 0.1 * repulse + 0.05 * struct

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch}: Training Loss = {total_loss / len(train_loader):.6f}")

        model.eval()
        with torch.no_grad():
            for i in range(min(3, len(val_set))):
                inc, gt = val_set[i]
                inc_input = inc.unsqueeze(0).to(device)
                pred, _ = model(inc_input)
                pred = pred[0].cpu()
                plot_pointclouds(inc, pred, gt, epoch, i)
        model.train()

#### Start continuation
continue_training(model, train_loader, val_set, start_epoch=21, end_epoch=50)

torch.save(model.state_dict(), '/content/pcn_trained_deep_skip_final_epoch50.pth')
print("Training complete. Model saved after 50 epochs.")

"""Visualize a few from training"""

import trimesh
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import open3d as o3d
import torch

####Load meshes
def load_mesh_from_file(path):
    return trimesh.load(path, force='mesh')

####Convert point cloud to mesh
def pointcloud_to_mesh(pc):
    if isinstance(pc, torch.Tensor):
        pc = pc.cpu().numpy()
    pc = pc.T

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pc)
    pcd.estimate_normals()
    pcd.orient_normals_consistent_tangent_plane(10)

    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8)
    mesh.compute_vertex_normals()
    return mesh

####Plot meshes side by side
def plot_meshes_and_pointclouds(inc_mesh, pred_mesh, gt_mesh, inc_pc, pred_pc, gt_pc, titles, save_path=None):
    fig = plt.figure(figsize=(15, 10))

    # ---------- Top row: Meshes ----------
    meshes = [inc_mesh, pred_mesh, gt_mesh]

    for i, mesh in enumerate(meshes):
        ax = fig.add_subplot(2, 3, i + 1, projection='3d')
        vertices = np.asarray(mesh.vertices)
        triangles = np.asarray(mesh.faces if hasattr(mesh, 'faces') else mesh.triangles)

        mesh_tris = [vertices[tri] for tri in triangles]
        collection = Poly3DCollection(
            mesh_tris,
            facecolor='lightgray',
            edgecolor='gray',
            linewidths=0.2,
            alpha=1.0
        )
        ax.add_collection3d(collection)
#0 1 2
        ax.auto_scale_xyz(vertices[:, 0], vertices[:, 1], vertices[:, 2])
        ax.set_title(titles[i] + " (Mesh)")
        ax.axis('off')
        ax.view_init(elev=30, azim=-60)

    # ---------- Bottom row: Point Clouds ----------
    pcs = [inc_pc, pred_pc, gt_pc]

    for i, pc in enumerate(pcs):
        ax = fig.add_subplot(2, 3, i + 4, projection='3d')
        if isinstance(pc, torch.Tensor):
            pc = pc.cpu().numpy()
        if pc.shape[0] == 3:
            pc = pc.T  # [N, 3]

        ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1, c='black')
        ax.set_title(titles[i] + " (Point Cloud)")
        ax.axis('off')
        ax.view_init(elev=30, azim=-60)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300)
        print(f"Saved plot to {save_path}")
    plt.show()


####Run prediction and plot
val_idx = 62

# Paths to the meshes
inc_mesh_path = incomplete_paths[val_idx]
gt_mesh_path = complete_paths[val_idx]

# Load meshes
inc_mesh = load_mesh_from_file(inc_mesh_path)
gt_mesh = load_mesh_from_file(gt_mesh_path)

# Point clouds for prediction
inc, gt = dataset[val_idx]
inc_input = inc.unsqueeze(0).to(device)

with torch.no_grad():
    pred, _ = model(inc_input)
    pred = pred[0].cpu()

# Predicted mesh
pred_mesh = pointcloud_to_mesh(pred)

# Plot meshes and point clouds
plot_meshes_and_pointclouds(
    inc_mesh, pred_mesh, gt_mesh,
    inc, pred, gt,
    ["Input (Incomplete)", "Prediction", "Ground Truth"],
    save_path=f"comparison_val_idx_{val_idx}.png"
)

"""test file visualization"""

!pip install open3d

!pip install -q gdown
import gdown
import trimesh
import glob
import torch
import open3d as o3d
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.art3d import Poly3DCollection


#### Setup functions
def normalize(pc):
    pc = pc - pc.mean(axis=1, keepdims=True)
    scale = pc.norm(dim=0).max()
    pc = pc / scale
    return pc

def sample_pointcloud_from_mesh(mesh_path, num_points=2048):
    mesh = trimesh.load(mesh_path, force='mesh')
    pts, _ = trimesh.sample.sample_surface(mesh, num_points)
    pc = torch.from_numpy(pts.T).float()
    pc = normalize(pc)
    return mesh, pc

def pointcloud_to_mesh(pc):
    if isinstance(pc, torch.Tensor):
        pc = pc.cpu().numpy()
    pc = pc.T
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pc)
    pcd.estimate_normals()
    pcd.orient_normals_consistent_tangent_plane(10)
    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8)
    mesh.compute_vertex_normals()
    return mesh

def plot_meshes_and_pointclouds(inc_mesh, pred_mesh, inc_pc, pred_pc, titles, save_path=None):
    fig = plt.figure(figsize=(10, 10))

    # ---------- Top row: Meshes ----------
    meshes = [inc_mesh, pred_mesh]

    for i, mesh in enumerate(meshes):
        ax = fig.add_subplot(2, 2, i + 1, projection='3d')
        vertices = np.asarray(mesh.vertices)
        triangles = np.asarray(mesh.faces if hasattr(mesh, 'faces') else mesh.triangles)

        mesh_tris = [vertices[tri] for tri in triangles]
        collection = Poly3DCollection(
            mesh_tris,
            facecolor='lightgray',
            edgecolor='gray',
            linewidths=0.2,
            alpha=1.0
        )
        ax.add_collection3d(collection)
        ax.auto_scale_xyz(vertices[:, 0], vertices[:, 1], vertices[:, 2])
        ax.set_title(titles[i] + " (Mesh)")
        ax.axis('off')
        ax.view_init(elev=30, azim=-60)

    # ---------- Bottom row: Point Clouds ----------
    pcs = [inc_pc, pred_pc]

    for i, pc in enumerate(pcs):
        ax = fig.add_subplot(2, 2, i + 3, projection='3d')
        if isinstance(pc, torch.Tensor):
            pc = pc.cpu().numpy()
        if pc.shape[0] == 3:
            pc = pc.T
        ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1, c='black')
        ax.set_title(titles[i] + " (Point Cloud)")
        ax.axis('off')
        ax.view_init(elev=30, azim=-60)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300)
        print(f"Saved plot to {save_path}")
    plt.show()


#### Step Download and unzip test zip

gdown.download(id="13CfWcetWhFKnNbioU1VUqFUcjlqW4BVv", output="test_meshes.zip", quiet=False)
!unzip -o test_meshes.zip -d test_meshes

#### Load meshes and sample point clouds

test_mesh_paths = sorted(glob.glob('/content/test_meshes/samples/*.glb'))
print("Found test meshes:", test_mesh_paths)

test_meshes = []
test_pointclouds = []

for path in test_mesh_paths:
    mesh, pc = sample_pointcloud_from_mesh(path)
    test_meshes.append(mesh)
    test_pointclouds.append(pc)


####Predict point clouds

pred_pointclouds = []

for pc in test_pointclouds:
    pc_input = pc.unsqueeze(0).to(device)
    with torch.no_grad():
        pred, _ = model(pc_input)
        pred = pred[0].cpu()
    pred_pointclouds.append(pred)


#### Convert predicted point clouds to meshes


pred_meshes = []

for pred_pc in pred_pointclouds:
    pred_mesh = pointcloud_to_mesh(pred_pc)
    pred_meshes.append(pred_mesh)


#### Visualize and save results

for i in range(len(test_meshes)):
    print(f"Visualizing Test Mesh #{i+1}")
    plot_meshes_and_pointclouds(
        test_meshes[i], pred_meshes[i],
        test_pointclouds[i], pred_pointclouds[i],
        ["Original", "Prediction"],
        save_path=f"test_mesh_{i+1}_comparison.png"
    )

"""SAve at 50 epochs:"""

# Save model
model_path = "/content/pcn_trained_deep_skip_final_epoch50.pth"
torch.save(model.state_dict(), model_path)

# Zip the model, cached pointclouds, and predicted meshes
!zip -r /content/pcn_epoch50_backup.zip /content/pcn_trained_deep_skip_final_epoch50.pth /content/cached_pointclouds /content/predicted_meshes_epoch50

# Copy to Google Drive
!cp /content/pcn_epoch50_backup.zip /content/drive/MyDrive/

print("Backup zip saved to your Google Drive at /MyDrive/pcn_epoch50_backup.zip")

"""Code to resume training later"""

!pip install trimesh

import os, glob, torch, trimesh
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split
import torch.nn as nn
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


#from google.colab import drive
#drive.mount('/content/drive')

# Copy zip from Google Drive back to Colab
!cp /content/drive/MyDrive/pcn_epoch50_backup.zip /content/

# Unzip it
!unzip -o /content/pcn_epoch50_backup.zip -d /content/

print("Backup restored.")


# Reload paths
incomplete_paths = sorted(glob.glob('/content/Mesh_Recreated_Holes/Mesh_Recreaded_Holes/*'))
complete_paths = sorted(glob.glob('/content/Mesh_Recreated_Original/Mesh_Recreated_Original/*'))

class CachedPointCloudDataset(Dataset):
    def __init__(self, incomplete_paths, complete_paths, num_points=2048, cache_dir="/content/cached_pointclouds"):
        self.incomplete_paths = incomplete_paths
        self.complete_paths = complete_paths
        self.num_points = num_points
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.inc_pc_paths, self.gt_pc_paths = self._prepare_cached_pointclouds()

    def _cache_exists_and_valid(self, path, expected_shape):
        return os.path.exists(path) and np.load(path).shape == expected_shape

    def _prepare_cached_pointclouds(self):
        inc_cached = []
        gt_cached = []
        expected_shape = (3, self.num_points)

        for inc_path, gt_path in zip(self.incomplete_paths, self.complete_paths):
            base = os.path.splitext(os.path.basename(inc_path))[0]
            inc_pc_file = os.path.join(self.cache_dir, f"{base}_inc.npy")
            gt_pc_file = os.path.join(self.cache_dir, f"{base}_gt.npy")

            if not self._cache_exists_and_valid(inc_pc_file, expected_shape):
                inc_mesh = trimesh.load(inc_path, force='mesh')
                inc_pts, _ = trimesh.sample.sample_surface(inc_mesh, self.num_points)
                np.save(inc_pc_file, inc_pts.T)

            if not self._cache_exists_and_valid(gt_pc_file, expected_shape):
                gt_mesh = trimesh.load(gt_path, force='mesh')
                gt_pts, _ = trimesh.sample.sample_surface(gt_mesh, self.num_points)
                np.save(gt_pc_file, gt_pts.T)

            inc_cached.append(inc_pc_file)
            gt_cached.append(gt_pc_file)

        return inc_cached, gt_cached

    def __len__(self):
        return len(self.inc_pc_paths)

    def __getitem__(self, idx):
        inc = torch.from_numpy(np.load(self.inc_pc_paths[idx])).float()
        gt = torch.from_numpy(np.load(self.gt_pc_paths[idx])).float()
        inc = normalize(inc)
        gt = normalize(gt)
        return inc, gt



class PCNEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Conv1d(3, 128, 1),
            nn.ReLU(),
            nn.Conv1d(128, 256, 1),
            nn.ReLU(),
            nn.Conv1d(256, 1024, 1),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.mlp(x)
        global_feat = torch.max(x, 2)[0]
        return global_feat

class FoldingDecoder(nn.Module):
    def __init__(self, grid_size=32, input_feat_dim=1024):
        super().__init__()
        self.num_fine = grid_size * grid_size
        self.grid = self.build_2d_grid(grid_size)
        self.mlp = nn.Sequential(
            nn.Conv1d(input_feat_dim + 2 + 3, 1024, 1),
            nn.ReLU(),
            nn.Conv1d(1024, 1024, 1),
            nn.ReLU(),
            nn.Conv1d(1024, 512, 1),
            nn.ReLU(),
            nn.Conv1d(512, 256, 1),
            nn.ReLU(),
            nn.Conv1d(256, 3, 1)
        )

    def build_2d_grid(self, grid_size):
        linspace = torch.linspace(-0.3, 0.3, grid_size)
        grid = torch.stack(torch.meshgrid(linspace, linspace, indexing='ij'), dim=-1).reshape(-1, 2)
        return grid

    def forward(self, coarse, global_feat):
        B, _, M = coarse.size()
        grid = self.grid.to(coarse.device).unsqueeze(0).repeat(B, 1, 1)
        grid = grid.permute(0, 2, 1)
        global_feat = global_feat.unsqueeze(-1).expand(-1, -1, self.num_fine)
        coarse_exp = coarse[:, :, :self.num_fine]
        x = torch.cat([grid, coarse_exp, global_feat], dim=1)
        fine = self.mlp(x) + 2.0 * coarse_exp  # Strong skip connection
        return fine



class PCN(nn.Module):
    def __init__(self, num_coarse=1024, grid_size=32):
        super().__init__()
        self.encoder = PCNEncoder()
        self.num_coarse = num_coarse
        self.grid_size = grid_size
        self.coarse_fc = nn.Sequential(
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Linear(1024, num_coarse * 3)
        )
        self.decoder = FoldingDecoder(grid_size=grid_size)

    def forward(self, x):
        global_feat = self.encoder(x)
        coarse = self.coarse_fc(global_feat).view(-1, 3, self.num_coarse)
        fine = self.decoder(coarse, global_feat)
        out = torch.cat([coarse, fine], dim=2)
        return out, coarse



#assert len(incomplete_paths) == len(complete_paths), "Mismatch between incomplete and complete lists!"

# Rebuild dataset using the cached pointclouds folder restored from zip
dataset = CachedPointCloudDataset(incomplete_paths, complete_paths, num_points=2048)

# Train/validation split
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_set, val_set = random_split(dataset, [train_size, val_size])
train_loader = DataLoader(train_set, batch_size=4, shuffle=True)

print("Dataset ready.")

# Create a new model instance
model = PCN(num_coarse=1024, grid_size=32).to(device)

# Load weights from the 50 epoch checkpoint
model.load_state_dict(torch.load('/content/content/pcn_trained_deep_skip_final_epoch50.pth'))
print("Model loaded from epoch 50 checkpoint.")

#continue_training(model, train_loader, val_set, start_epoch=51, end_epoch=70)  # Example to go to 70

#torch.save(model.state_dict(), '/content/pcn_trained_deep_skip_final_epoch70.pth')
#print("Training complete. Model saved at epoch 70.")

"""If generating new data to test/train: Visualize ground truth and point clouds with holes to verify no mismatch:"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def plot_pair(inc, gt, idx):
    fig = plt.figure(figsize=(8, 4))

    ax1 = fig.add_subplot(1, 2, 1, projection='3d')
    pc = inc.T
    ax1.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
    ax1.set_title(f'Input with Hole #{idx}')
    ax1.axis('off')

    ax2 = fig.add_subplot(1, 2, 2, projection='3d')
    pc = gt.T
    ax2.scatter(pc[:, 0], pc[:, 1], pc[:, 2], s=1)
    ax2.set_title('Ground Truth')
    ax2.axis('off')

    plt.tight_layout()
    plt.show()

# Visualize all pairs
for idx in range(len(dataset)):
    inc, gt = dataset[idx]
    plot_pair(inc, gt, idx)