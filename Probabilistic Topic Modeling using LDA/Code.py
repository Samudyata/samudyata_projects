# -*- coding: utf-8 -*-
"""Final_SML_MIdterm_Update.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1il_lAbPve3sGKe--FeyDdYnTcwZ7hMah

## IMPORTING DATASET AND LIBRARIES
"""

!pip install --upgrade --force-reinstall numpy gensim

#1. Run this PIP INSTALL
#2. RESTART SESSION(IT WILL ASK AUTOMATICALLY)
#3. COMMENT THE PIP INSTALL
#4. RUN REST OF THE CODE

"""**Make Sure to add all the files in FinalRequirements Folder to Google Colab**"""

import nltk
nltk.download('stopwords')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import string
import seaborn as sns
import time

# nltk
import nltk
from nltk.corpus import stopwords
stoplist= stopwords.words('english')
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer= WordNetLemmatizer()
from nltk.corpus import wordnet
from sklearn.feature_extraction.text import CountVectorizer

import warnings
warnings.filterwarnings('ignore')

# Enable logging
import logging
logging.basicConfig(level= logging.INFO)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("nicapotato/womens-ecommerce-clothing-reviews")

print("Path to dataset files:", path)

import os
import pandas as pd

# Define the dataset path
dataset_path = "/kaggle/input/womens-ecommerce-clothing-reviews"
csv_path = os.path.join(dataset_path, "Womens Clothing E-Commerce Reviews.csv")

# Load the dataset
df = pd.read_csv(csv_path, index_col=0)

# Clean column names
df.columns = df.columns.str.replace(" ", "_")

# Display the first few rows of the dataframe
print(df.head())

"""# FEATURE ENGINEERING"""

# Since the reviews is our main content, dropping rows where 'Review Text' is null
df.dropna(subset=['Review_Text'], inplace=True)
df.shape

# Review word count
df['rev_word_count']= df['Review_Text'].apply(lambda x: len(x.strip().split()))

# Unique word count
df['unique_word_count']= df['Review_Text'].apply(lambda x: len(set(str(x).split())))

# Bucketing Clothing ID's with 1 or 2 count

clothing_id_to_combine=[]
for val, cnt in df.Clothing_ID.value_counts().items():
    # If that Clothing_ID is present less than 1%(~200) of the total data, club it into '000' (default) id
    if(cnt<200):
        clothing_id_to_combine.append(val)

print("# of clothing ID's clubbed: ",len(clothing_id_to_combine))

df['new_clothingID']= df.Clothing_ID.apply(lambda x: '000' if x in clothing_id_to_combine else x)
df.new_clothingID.value_counts(normalize=True)

"""# SENTIMENT ANALYSIS"""

pip install vaderSentiment

import vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer= SentimentIntensityAnalyzer()

vs= analyzer.polarity_scores("Vader sentiment looks interesting, I have high hopes!")
print(vs)

df['review_sentiment']= [analyzer.polarity_scores(line)['compound'] for line in df['Review_Text']]

df.Title.fillna('no title', inplace=True)
df['title_sentiment']= df['Title'].apply(lambda x: analyzer.polarity_scores(x)['compound'] if str(x)!= 'no title' else 0.0)

for index, row in df[100:120].iterrows():
    print(row['Title']," >>>>>", row['title_sentiment'])

df['total_sentiment_score']= df['title_sentiment']+ df['review_sentiment']

df_orig= df.copy()
df_orig.shape

"""# Feature Encoding"""

df.drop(columns=['Review_Text','Title','Clothing_ID','review_sentiment','title_sentiment'], inplace=True)

cat_cols= ['Division_Name','Department_Name','Class_Name','new_clothingID']
for col in cat_cols:
    print(col," has categories:", df[col].nunique())
    df[col]= df[col].astype('category')

pip install category_encoders

import category_encoders as ce

be= ce.BinaryEncoder(cols= cat_cols,drop_invariant=True).fit(df)

df= be.transform(df)

"""# Clustering"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from scipy import stats
from sklearn.cluster import KMeans
import pylab as pl
# %matplotlib inline
import matplotlib.pyplot as plt

import tensorflow as tf
# Ensure that TensorFlow uses GPU
if tf.config.list_physical_devices('GPU'):
    print("TensorFlow is using GPU.")
else:
    print("TensorFlow is using CPU.")

#MODEL TRAINED, LOAD IT
"""
pca_tsne= Pipeline([("pca", PCA(n_components= 0.90, random_state=33)),
                    ("tsne", TSNE(n_components=2,
                                  perplexity= 170,
                                  random_state=33,
                                  learning_rate= 350,
                                  n_iter= 5000,
                                  n_jobs=-1,
                                  n_iter_without_progress=150,
                                  verbose=1))])
t0= time.time()
df_pca_tsne_reduced= pca_tsne.fit_transform(df)
t1= time.time()

print("pca+tsne took:{:.1f}s ".format(t1-t0))
"""

import joblib
import pandas as pd
import numpy as np

# Load the trained pipeline
pca_tsne = joblib.load("pipeline_pca_tsne.pkl")

# Load the reduced 2D data
df_pca_tsne_reduced = joblib.load("pca_tsne_5000.pkl")
df_pca_tsne_reduced = pd.DataFrame(df_pca_tsne_reduced, columns=["TSNE1", "TSNE2"])
type(df_pca_tsne_reduced)

import matplotlib.pyplot as plt

plt.scatter(df_pca_tsne_reduced["TSNE1"], df_pca_tsne_reduced["TSNE2"], s=5)
plt.title("t-SNE Projection")
plt.xlabel("TSNE1")
plt.ylabel("TSNE2")
plt.grid(True)
plt.show()

#To save the model trained
"""
import joblib

# Save only the reduced matrix
joblib.dump(df_pca_tsne_reduced, "pca_tsne_5000.pkl")

# Save the entire pipeline (optional)
joblib.dump(pca_tsne, "pipeline_pca_tsne.pkl")
"""

"""import seaborn as sns
import matplotlib.pyplot as plt

sns.set(rc={'figure.figsize': (13, 13)})

# Assuming df_pca_tsne_reduced is a 2D numpy array or DataFrame with at least two columns
sns.scatterplot(x=df_pca_tsne_reduced[:, 0], y=df_pca_tsne_reduced[:, 1])

plt.show()

# Kmeans
"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import time

# Define the range of clusters to evaluate
number_of_clusters = range(2, 11)

t0 = time.time()

kmeans = [KMeans(n_clusters=i, max_iter=1000, random_state=33, n_init=10) for i in number_of_clusters]

# Calculate the score (negative inertia) for each KMeans model
score = [-1 * kmeans[i].fit(df_pca_tsne_reduced).score(df_pca_tsne_reduced) for i in range(len(kmeans))]

t1 = time.time()
print(f"Time taken: {t1 - t0} seconds")

from sklearn.cluster import KMeans
import time
import matplotlib.pyplot as plt

t0 = time.time()
number_of_clusters = range(1, 20)

kmeans = [KMeans(n_clusters=i, max_iter=1000, random_state=33) for i in number_of_clusters]
score = [-1 * kmeans[i].fit(df_pca_tsne_reduced).score(df_pca_tsne_reduced) for i in range(len(kmeans))]
t1 = time.time()

plt.plot(number_of_clusters, score, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()

print("Plotting the Elbow curve took: {:.1f}s".format(t1 - t0))

from sklearn.metrics import silhouette_score

sil_scores = []
for i in range(2, 11):  # Start from 2 for silhouette
    kmeans = KMeans(n_clusters=i, random_state=33, n_init=20, max_iter=1000)
    labels = kmeans.fit_predict(df_pca_tsne_reduced)
    sil = silhouette_score(df_pca_tsne_reduced, labels)
    sil_scores.append(sil)
    print(f"k={i+1}, Silhouette Score={sil:.4f}")

from sklearn.cluster import KMeans

# Create KMeans object with correct parameters
k_means_test = KMeans(n_clusters=3, max_iter=1500, random_state=33, verbose=1)

# Fitting the model
-1 * k_means_test.fit(df_pca_tsne_reduced).score(df_pca_tsne_reduced)

# Get the predicted labels
y_pred = k_means_test.labels_

print(y_pred)

import numpy as np

df['klabels'] = k_means_test.labels_

size_of_each_cluster = df.groupby('klabels').size().reset_index()
size_of_each_cluster.columns = ['klabels', 'number_of_points']
size_of_each_cluster['percentage'] = (size_of_each_cluster['number_of_points'] / np.sum(size_of_each_cluster['number_of_points'])) * 100

print(size_of_each_cluster)

import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
sns.set(rc={'figure.figsize': (13, 13)})

sns.scatterplot(
    x=df_pca_tsne_reduced["TSNE1"],
    y=df_pca_tsne_reduced["TSNE2"],
    hue=y_pred,
    palette='Set1'
)

plt.title("t-SNE with KMeans labels")
plt.xlabel("TSNE1")
plt.ylabel("TSNE2")
plt.show()

"""#only if ran completely
import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
sns.set(rc={'figure.figsize': (13, 13)})

# Scatter plot with the first two columns of df_pca_tsne_reduced and cluster labels
sns.scatterplot(x=df_pca_tsne_reduced[:, 0], y=df_pca_tsne_reduced[:, 1], hue=y_pred, palette='Set1')

plt.title("t-SNE with KMeans labels")
plt.show()

# LDA
"""

import nltk
import string
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Download all necessary resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')
try:
    nltk.download('punkt_tab')
    nltk.download('averaged_perceptron_tagger_eng')
except:
    pass

# Initialize tools
stoplist = set(stopwords.words('english'))
wordnet_lemmatizer = WordNetLemmatizer()

def get_pos_tag(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun

def preprocess(text):
    if pd.isna(text) or not isinstance(text, str):
        return []

    # Simple tokenization
    punctuation = list(string.punctuation)

    # Split by whitespace and clean manually
    tokens = text.lower().split()
    # Remove punctuation and short words
    tokens = [''.join(c for c in token if c not in punctuation) for token in tokens]
    tokens = [token for token in tokens if len(token) > 3]

    # Simple lemmatization without POS tagging
    lemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]

    # Remove stopwords
    filtered_words = [word for word in lemmatized if word not in stoplist]

    return filtered_words

try:
    df_clean = df_orig['Review_Text'].apply(preprocess)
    print(df_clean.head())
except NameError:
    print("df_orig not found. Creating a sample DataFrame for testing:")
    sample_df = pd.DataFrame({
        'Review_Text': [
            "This product is amazing! I love it so much.",
            "Not worth the money. Very disappointed with quality.",
            "The service was excellent, would recommend to friends."
        ]
    })
    df_clean = sample_df['Review_Text'].apply(preprocess)
    print(df_clean)
except KeyError:
    print("'Review_Text' column not found in df_orig.")

common_terms= ["wear","look","ordered","color","purchase","order"]

stoplist = stoplist.union(set(common_terms))

def get_nouns_adjs(series):

    " Topic Modeling using only nouns and adjectives"

    pos_tags= nltk.pos_tag(series)
    all_adj_nouns= [word for (word, tag) in pos_tags if (tag=="NN" or tag=="NNS" or tag=="JJ")]
    return all_adj_nouns

df_nouns_adj = df_clean.apply(get_nouns_adjs)

import gensim
import gensim.models

docs= list(df_nouns_adj)
phrases = gensim.models.Phrases(docs, min_count=10, threshold=20)
bigram_model = gensim.models.phrases.Phraser(phrases)

def make_bigrams(texts):
    return [bigram_model[doc] for doc in texts]

# Form Bigrams
data_words_bigrams = make_bigrams(docs)

from gensim.models import Word2Vec
from collections import Counter

# Train Word2Vec with only hierarchical softmax
w2vmodel = Word2Vec(
    bigram_model[docs],
    vector_size=100,
    sg=1,
    hs=1,
    negative=0,
    seed=33,
    epochs=35
)
# Checkout most frequent bigrams :
bigram_counter1= Counter()
for key in phrases.vocab.keys():
    if key not in stopwords.words('english'):
        if len(str(key).split('_'))>1:
            bigram_counter1[key]+=phrases.vocab[key]

for key, counts in bigram_counter1.most_common(20):
    print(key,">>>>", counts)

"""Feeding the bigrams into a Word2Vec model produces more meaningful bigrams"""

# Which color is to 'work' as 'white' is to 'wedding'
w2vmodel.wv.most_similar(['work','white'], ['wedding'], topn=5)

w2vmodel.wv.most_similar(['price','steal'], ['discount'], topn=5)

# What is a 'deal_breaker', if 'quality'is 'worth_penny'
w2vmodel.wv.most_similar(positive=["deal_breaker","quality"], negative=["worth_penny"], topn=3)

"""Filter out uncommon words"""

from gensim.corpora import Dictionary

# Assume data_words_bigrams is your tokenized + bigrammed dataset
dictionary = Dictionary(data_words_bigrams)

# Filter: remove very rare and very common words
dictionary.filter_extremes(no_below=20, no_above=0.6)

# Create the Bag-of-Words representation
corpus = [dictionary.doc2bow(doc) for doc in data_words_bigrams]

print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))

"""Training LDA"""

#ALREADY TRAINED, LOAD THE MODEL INSTEAD
"""
from gensim.models.ldamulticore import LdaMulticore

t0= time.time()
passes= 150
np.random.seed(1) # setting up random seed to get the same results
ldamodel= LdaMulticore(corpus,
                    id2word=dictionary,
                    num_topics=4,
#                   alpha='asymmetric',
                    chunksize= 4000,
                    batch= True,
                    minimum_probability=0.001,
                    iterations=350,
                    passes=passes)

t1= time.time()
print("time for",passes," passes: ",(t1-t0)," seconds")
"""

#TO SAVE THE MODEL
"""
ldamodel.save("lda_model_4topics_pass150.gensim")
import pickle

# Save dictionary
dictionary.save("lda_dictionary.dict")

# Save corpus
with open("lda_corpus.pkl", "wb") as f:
    pickle.dump(corpus, f)
"""

from gensim.models.ldamulticore import LdaMulticore

ldamodel = LdaMulticore.load("lda_model_4topics_pass150.gensim")

from gensim.corpora import Dictionary

dictionary = Dictionary.load("lda_dictionary.dict")

import pickle

with open("lda_corpus.pkl", "rb") as f:
    corpus = pickle.load(f)

ldamodel.show_topics(num_words=25, formatted=False)

lda_corpus= ldamodel[corpus]

# Obtaining the main topic for each review:

all_topics = ldamodel.get_document_topics(corpus)
num_docs = len(all_topics)

all_topics_csr= gensim.matutils.corpus2csc(all_topics)
all_topics_numpy= all_topics_csr.T.toarray()

major_topic= [np.argmax(arr) for arr in all_topics_numpy]
df_orig['major_lda_topic']= major_topic

sns.set(rc= {'figure.figsize': (5,3)})
sns.set_style('darkgrid')

df_orig.major_lda_topic.value_counts().plot(kind='bar')

df_orig["klabels"] = y_pred

df_orig.groupby('klabels')['major_lda_topic'].value_counts(normalize=True, ascending=False)

num_cols= ['Age','Positive_Feedback_Count','rev_word_count', 'unique_word_count','total_sentiment_score']

cat_cols= ['major_lda_topic','Division_Name','Department_Name','Class_Name']

cluster1= df_orig.loc[(df_orig.klabels==0)]
cluster2= df_orig.loc[(df_orig.klabels==1)]
cluster3= df_orig.loc[(df_orig.klabels==2)]

pd.DataFrame((cluster1.Rating.value_counts()*100)/df_orig.Rating.value_counts()).plot(kind='bar')

print('Visualizing numerical features:')
for i, col in enumerate(num_cols):
    plt.figure(i)
    sns.distplot(cluster1[col])

print('Visualizing categorical features:')
for i, col in enumerate(cat_cols):
    plt.figure(i)
    chart= sns.countplot(cluster1[col], order= cluster1[col].value_counts().index)
    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)

# **Cluster 2 Analysis**
print('Visualizing numerical features:')
for i, col in enumerate(num_cols):
    plt.figure(i)
    sns.distplot(cluster2[col])

pd.DataFrame((cluster2.Rating.value_counts()*100)/df_orig.Rating.value_counts()).plot(kind='bar')

print('Visualizing categorical features:')
for i, col in enumerate(cat_cols):
    plt.figure(i)
    chart= sns.countplot(cluster2[col], order= cluster2[col].value_counts().index)
    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)

# Cluster 3 Analysis
pd.DataFrame((cluster3.Rating.value_counts()*100)/df_orig.Rating.value_counts()).plot(kind='bar')

print('Visualizing numerical features:')
for i, col in enumerate(num_cols):
    plt.figure(i)
    sns.distplot(cluster3[col])

print('Visualizing categorical features:')
for i, col in enumerate(cat_cols):
    plt.figure(i)
    chart= sns.countplot(cluster3[col], order= cluster3[col].value_counts().index)
    chart.set_xticklabels(chart.get_xticklabels(),rotation=90)

topics = ldamodel.show_topics(num_topics=4, num_words=10, formatted=False)

for topic_num, word_probs in topics:
    print(f"\nTopic {topic_num}:")
    print(", ".join([word for word, prob in word_probs]))

import numpy as np
from scipy.spatial.distance import jensenshannon

# -----------------------------
# 1. Perplexity
# -----------------------------
perplexity = np.exp2(-ldamodel.log_perplexity(corpus))
print(f"Model Perplexity: {perplexity:.4f}")

# -----------------------------
# 2. Pairwise Jensen-Shannon Divergence
# -----------------------------
topics = ldamodel.get_topics()
num_topics = topics.shape[0]

print("\nPairwise Jensen-Shannon Divergence between Topics:")
for i in range(num_topics):
    for j in range(i + 1, num_topics):
        js_div = jensenshannon(topics[i], topics[j])**2
        print(f"JS Divergence (Topic {i} vs {j}): {js_div:.4f}")